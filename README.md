# ğŸ§  Lead Data Engineer Technical Assessment (English Version)

This repository contains the complete solution for the Lead Data Engineer technical assessment. It includes data processing, API integration, PostgreSQL loading, analytical queries, performance strategies, and architectural proposals.


Data-Engineer-Lead/
â”‚
â”œâ”€â”€ data/                   # Original CSV files
â”‚   â”œâ”€â”€ orders.csv
â”‚   â””â”€â”€ products.csv
â”‚
â”œâ”€â”€ outputs/                # Generated outputs
â”‚   â”œâ”€â”€ order_full_information.csv
â”‚   â”œâ”€â”€ fixed_order_full_information.csv
â”‚   â””â”€â”€ kpi_product_orders.csv
â”‚
â”œâ”€â”€ scripts/                # Python and SQL scripts
â”‚   â”œâ”€â”€ challenge_1_merge_csv.py
â”‚   â”œâ”€â”€ challenge_2_currency_conversion.py
â”‚   â”œâ”€â”€ challenge_2_kpi_analysis.py
â”‚   â”œâ”€â”€ challenge_3_postgres_loader.py
â”‚   â”œâ”€â”€ challenge_3_create_indexes.sql
â”‚   â”œâ”€â”€ challenge_3_kpi_queries.sql
â”‚   â””â”€â”€ challenge_4_proposal.md
â”‚
â”œâ”€â”€ .env                    # Environment variables (excluded from Git)
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md               # This file



ğŸ§ Challenge 1: Merging Orders and Products Data

âœ… Goal:

Merge orders.csv and products.csv to generate order_full_information.csv with:

order_created_date

order_id

product_name

quantity

total_price (calculated as price * quantity)

ğŸ› ï¸ Tools:

Python

pandas

ğŸ“ƒ Implementation:

Load both CSVs

Merge on product_id

Calculate total price

Format columns and export CSV

âœ¨ Improvements:

Validate missing or orphan product_id

Use type hints and logging for production readiness

ğŸŒŸ Challenge 2: Currency Conversion and KPIs

âœ… Part A: BRL to USD Conversion

API: freecurrencyapi.com

API key loaded via .env

Generated fixed_order_full_information.csv with:

total_price_br

total_price_us (using real-time rate)

âœ… Part B: KPIs Generation

Produced kpi_product_orders.csv with:

Date with the highest number of orders

Most demanded product + total sales in USD

Top 3 most demanded categories

ğŸ¤ Techniques:

pandas groupby, sum, merge

CSV export via to_csv()

âœ¨ Improvements:

Use unit tests to validate metrics

Generate dashboards via Jupyter or Tableau

ğŸ§° Challenge 3: PostgreSQL Loading + SQL Analysis

âœ… Part A: Programmatic Ingestion

PostgreSQL used locally with SQLAlchemy

Data loaded from CSV into products and orders

Used FOREIGN KEY (product_id) REFERENCES products(product_id) to ensure referential integrity

âœ… Part B: SQL KPIs

SQL file challenge_3_kpi_queries.sql includes:

Max order date

Most demanded product with ROUND() conversion fix

Top 3 categories

ğŸ“ˆ Performance Optimizations:

Created indexes on created_date, product_id, category

Recommended materialized views for heavy aggregations

Used LIMIT to reduce scan overhead

Suggest table partitioning by date in large datasets

ğŸŒ Governance Considerations:

Add metadata table for refresh timestamps

Enforce data typing with column constraints

Use schema versioning for evolutive changes

âœ¨ Improvements:

Validate integrity with triggers

Schedule data refresh via Airflow or cron

ğŸ¤– Challenge 4: Strategic Proposal

Contained in challenge_4_proposal.md. Summary:

âœ¨ Additional Business Insights:

Revenue per day (trend detection)

Average ticket per order

Product with highest day-to-day growth

âš–ï¸ ELT Architecture:

Tool: Airbyte + dbt + BigQuery

Flow: Extract (Postgres) â†’ Load (BigQuery) â†’ Transform (dbt)

ğŸ§  AI-Based Pipeline:

Model: Demand forecasting with regression (XGBoost or sklearn)

Input features: date, category, price, lag features

Output: Demand per product for X days

Deployment: Vertex AI Pipelines or SageMaker

ğŸš€ Project Setup

# Create and activate virtual environment
python -m venv venv
venv\Scripts\activate  # On Windows

# Install dependencies
pip install -r requirements.txt

ğŸ“§ Author

Felipe OlivaresGitHub: @FelipeOlivaresM